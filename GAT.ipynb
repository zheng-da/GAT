{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from torch import nn\n",
    "import dgl\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Mapping\n",
    "import dgl.function as fn\n",
    "\n",
    "def expand_as_pair(input_, g=None):\n",
    "    \"\"\"Return a pair of same element if the input is not a pair.\n",
    "\n",
    "    If the graph is a block, obtain the feature of destination nodes from the source nodes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_ : Tensor, dict[str, Tensor], or their pairs\n",
    "        The input features\n",
    "    g : DGLHeteroGraph or DGLGraph or None\n",
    "        The graph.\n",
    "\n",
    "        If None, skip checking if the graph is a block.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[Tensor, Tensor] or tuple[dict[str, Tensor], dict[str, Tensor]]\n",
    "        The features for input and output nodes\n",
    "    \"\"\"\n",
    "    if isinstance(input_, tuple):\n",
    "        return input_\n",
    "    elif g is not None and g.is_block:\n",
    "        if isinstance(input_, Mapping):\n",
    "            input_dst = {\n",
    "                k: v[0:g.number_of_dst_nodes(k)]\n",
    "                for k, v in input_.items()}\n",
    "        else:\n",
    "            input_dst = input_[0:g.number_of_dst_nodes()]\n",
    "        return input_, input_dst\n",
    "    else:\n",
    "        return input_, input_\n",
    "\n",
    "class GATConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 out_feats,\n",
    "                 num_heads,\n",
    "                 feat_drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 negative_slope=0.2,\n",
    "                 residual=False,\n",
    "                 activation=None,\n",
    "                 allow_zero_in_degree=False,\n",
    "                 bias=True):\n",
    "        super(GATConv, self).__init__()\n",
    "        self._num_heads = num_heads\n",
    "        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n",
    "        self._out_feats = out_feats\n",
    "        self._allow_zero_in_degree = allow_zero_in_degree\n",
    "        if isinstance(in_feats, tuple):\n",
    "            self.fc_src = nn.Linear(\n",
    "                self._in_src_feats, out_feats * num_heads, bias=False)\n",
    "            self.fc_dst = nn.Linear(\n",
    "                self._in_dst_feats, out_feats * num_heads, bias=False)\n",
    "        else:\n",
    "            self.fc = nn.Linear(\n",
    "                self._in_src_feats, out_feats * num_heads, bias=False)\n",
    "        self.attn_fc = nn.Parameter(th.FloatTensor(size=(1, num_heads, out_feats * 2)))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.feat_drop = nn.Dropout(feat_drop)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(th.FloatTensor(size=(num_heads * out_feats,)))\n",
    "        else:\n",
    "            self.register_buffer('bias', None)\n",
    "        if residual:\n",
    "            if self._in_dst_feats != out_feats:\n",
    "                self.res_fc = nn.Linear(\n",
    "                    self._in_dst_feats, num_heads * out_feats, bias=False)\n",
    "            else:\n",
    "                self.res_fc = Identity()\n",
    "        else:\n",
    "            self.register_buffer('res_fc', None)\n",
    "        self.reset_parameters()\n",
    "        self.activation = activation\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        if hasattr(self, 'fc'):\n",
    "            nn.init.xavier_normal_(self.fc.weight, gain=gain)\n",
    "        else:\n",
    "            nn.init.xavier_normal_(self.fc_src.weight, gain=gain)\n",
    "            nn.init.xavier_normal_(self.fc_dst.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.attn_fc, gain=gain)\n",
    "        nn.init.constant_(self.bias, 0)\n",
    "        if isinstance(self.res_fc, nn.Linear):\n",
    "            nn.init.xavier_normal_(self.res_fc.weight, gain=gain)\n",
    "            \n",
    "    def forward(self, graph, feat, get_attention=False):\n",
    "        with graph.local_scope():\n",
    "            if isinstance(feat, tuple):\n",
    "                h_src = self.feat_drop(feat[0])\n",
    "                h_dst = self.feat_drop(feat[1])\n",
    "                if not hasattr(self, 'fc_src'):\n",
    "                    feat_src = self.fc(h_src).view(-1, self._num_heads, self._out_feats)\n",
    "                    feat_dst = self.fc(h_dst).view(-1, self._num_heads, self._out_feats)\n",
    "                else:\n",
    "                    feat_src = self.fc_src(h_src).view(-1, self._num_heads, self._out_feats)\n",
    "                    feat_dst = self.fc_dst(h_dst).view(-1, self._num_heads, self._out_feats)\n",
    "            else:\n",
    "                h_src = h_dst = self.feat_drop(feat)\n",
    "                tmp = self.fc(h_src)\n",
    "                feat_src = feat_dst = tmp.view(\n",
    "                    -1, self._num_heads, self._out_feats)\n",
    "                if graph.is_block:\n",
    "                    feat_dst = feat_src[:graph.number_of_dst_nodes()]\n",
    "                    \n",
    "            def msg_func(edges):\n",
    "                d = th.cat([edges.src['ft'], edges.dst['ft']], dim=-1)\n",
    "                e = (d * self.attn_fc).sum(dim=-1)\n",
    "                e = self.leaky_relu(e)\n",
    "                return {'m': self.softmax(e), 'ft': edges.src['ft']}\n",
    "            def reduce_func(nodes):\n",
    "                a = self.attn_drop(self.softmax(nodes.mailbox['m']).unsqueeze(-1))\n",
    "                data = (a * nodes.mailbox['ft']).sum(dim=1).reshape(nodes.batch_size(), -1)\n",
    "                return {'h': data}\n",
    "            graph.srcdata.update({'ft': feat_src})\n",
    "            graph.dstdata.update({'ft': feat_dst})\n",
    "            graph.update_all(msg_func, reduce_func)\n",
    "            rst = graph.dstdata['h']\n",
    "            rst = rst.view(rst.shape[0], self._num_heads, self._out_feats)\n",
    "            # residual\n",
    "            if self.res_fc is not None:\n",
    "                resval = self.res_fc(h_dst).view(h_dst.shape[0], self._num_heads, self._out_feats)\n",
    "                rst = rst + resval\n",
    "            # bias\n",
    "            if self.bias is not None:\n",
    "                rst = rst + self.bias.view(1, self._num_heads, self._out_feats)\n",
    "            # activation\n",
    "            if self.activation:\n",
    "                rst = self.activation(rst)\n",
    "                \n",
    "            if get_attention:\n",
    "                return rst, graph.edata['a']\n",
    "            else:\n",
    "                return rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 n_hidden,\n",
    "                 n_classes,\n",
    "                 num_heads,\n",
    "                 n_layers,\n",
    "                 activation,\n",
    "                 feat_drop,\n",
    "                 attn_drop):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(GATConv(in_feats, n_hidden, num_heads, feat_drop=feat_drop, attn_drop=attn_drop,\n",
    "                                   activation=activation, residual=False))\n",
    "        for i in range(1, n_layers - 1):\n",
    "            self.layers.append(GATConv(n_hidden * num_heads, n_hidden, num_heads, feat_drop=feat_drop, attn_drop=attn_drop,\n",
    "                                       activation=activation, residual=False))\n",
    "        self.layers.append(GATConv(n_hidden * num_heads, n_classes, num_heads, feat_drop=feat_drop, attn_drop=attn_drop,\n",
    "                                   activation=None, residual=False))\n",
    "\n",
    "    def forward(self, blocks, x):\n",
    "        h = x\n",
    "        for l, (layer, block) in enumerate(zip(self.layers, blocks)):\n",
    "            if l < self.n_layers - 1:\n",
    "                h = layer(block, h).flatten(1)\n",
    "            else:\n",
    "                h = layer(block, h).mean(1)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_graph import load_reddit, load_ogb\n",
    "\n",
    "gpu = -1\n",
    "dataset = 'reddit'\n",
    "\n",
    "if gpu >= 0:\n",
    "    device = th.device('cuda:%d' % args.gpu)\n",
    "else:\n",
    "    device = th.device('cpu')\n",
    "\n",
    "if dataset == 'reddit':\n",
    "    g, n_classes = load_reddit()\n",
    "elif dataset == 'ogbn-products':\n",
    "    g, n_classes = load_ogb('ogbn-products')\n",
    "else:\n",
    "    raise Exception('unknown dataset')\n",
    "\n",
    "train_g = val_g = test_g = g\n",
    "train_nfeat = val_nfeat = test_nfeat = g.ndata.pop('features')\n",
    "train_labels = val_labels = test_labels = g.ndata.pop('labels')\n",
    "\n",
    "# Pack data\n",
    "data = n_classes, train_g, val_g, test_g, train_nfeat, train_labels, \\\n",
    "           val_nfeat, val_labels, test_nfeat, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc(pred, labels):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of prediction given the labels.\n",
    "    \"\"\"\n",
    "    labels = labels.long()\n",
    "    return (th.argmax(pred, dim=1) == labels).float().sum() / len(pred)\n",
    "\n",
    "def evaluate(model, g, nfeat, labels, val_nid, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation set specified by ``val_nid``.\n",
    "    g : The entire graph.\n",
    "    inputs : The features of all the nodes.\n",
    "    labels : The labels of all the nodes.\n",
    "    val_nid : the node Ids for validation.\n",
    "    device : The GPU device to evaluate on.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        pred = model.inference(g, nfeat, device)\n",
    "    model.train()\n",
    "    return compute_acc(pred[val_nid], labels[val_nid].to(pred.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fanout = [10, 10]\n",
    "batch_size = 1000\n",
    "num_workers = 4\n",
    "num_hidden = 16\n",
    "num_heads = 2\n",
    "num_layers = len(fanout)\n",
    "dropout = 0.5\n",
    "lr = 0.003\n",
    "num_epochs = 10\n",
    "log_every = 20\n",
    "eval_every = 100\n",
    "\n",
    "def load_subtensor(nfeat, labels, seeds, input_nodes, device):\n",
    "    \"\"\"\n",
    "    Extracts features and labels for a subset of nodes\n",
    "    \"\"\"\n",
    "    batch_inputs = nfeat[input_nodes].to(device)\n",
    "    batch_labels = labels[seeds].to(device)\n",
    "    return batch_inputs, batch_labels\n",
    "\n",
    "def run(device, data):\n",
    "    # Unpack data\n",
    "    n_classes, train_g, val_g, test_g, train_nfeat, train_labels, \\\n",
    "    val_nfeat, val_labels, test_nfeat, test_labels = data\n",
    "    in_feats = train_nfeat.shape[1]\n",
    "    train_nid = th.nonzero(train_g.ndata['train_mask'], as_tuple=True)[0]\n",
    "    val_nid = th.nonzero(val_g.ndata['val_mask'], as_tuple=True)[0]\n",
    "    test_nid = th.nonzero(~(test_g.ndata['train_mask'] | test_g.ndata['val_mask']), as_tuple=True)[0]\n",
    "\n",
    "    dataloader_device = th.device('cpu')\n",
    "\n",
    "    # Create PyTorch DataLoader for constructing blocks\n",
    "    sampler = dgl.dataloading.MultiLayerNeighborSampler(fanout)\n",
    "    dataloader = dgl.dataloading.NodeDataLoader(\n",
    "        train_g,\n",
    "        train_nid,\n",
    "        sampler,\n",
    "        device=dataloader_device,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        num_workers=num_workers)\n",
    "\n",
    "    # Define model and optimizer\n",
    "    model = GAT(in_feats, num_hidden, n_classes, num_heads, num_layers, F.relu, dropout, dropout)\n",
    "    model = model.to(device)\n",
    "    loss_fcn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Training loop\n",
    "    avg = 0\n",
    "    iter_tput = []\n",
    "    for epoch in range(num_epochs):\n",
    "        tic = time.time()\n",
    "\n",
    "        # Loop over the dataloader to sample the computation dependency graph as a list of\n",
    "        # blocks.\n",
    "        tic_step = time.time()\n",
    "        for step, (input_nodes, seeds, blocks) in enumerate(dataloader):\n",
    "            # Load the input features as well as output labels\n",
    "            batch_inputs, batch_labels = load_subtensor(train_nfeat, train_labels,\n",
    "                                                        seeds, input_nodes, device)\n",
    "            blocks = [block.int().to(device) for block in blocks]\n",
    "\n",
    "            # Compute loss and prediction\n",
    "            batch_pred = model(blocks, batch_inputs)\n",
    "            loss = loss_fcn(batch_pred, batch_labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            iter_tput.append(len(seeds) / (time.time() - tic_step))\n",
    "            if step % log_every == 0:\n",
    "                acc = compute_acc(batch_pred, batch_labels)\n",
    "                gpu_mem_alloc = th.cuda.max_memory_allocated() / 1000000 if th.cuda.is_available() else 0\n",
    "                print('Epoch {:05d} | Step {:05d} | Loss {:.4f} | Train Acc {:.4f} | Speed (samples/sec) {:.4f} | GPU {:.1f} MB'.format(\n",
    "                    epoch, step, loss.item(), acc.item(), np.mean(iter_tput[3:]), gpu_mem_alloc))\n",
    "            tic_step = time.time()\n",
    "\n",
    "        toc = time.time()\n",
    "        print('Epoch Time(s): {:.4f}'.format(toc - tic))\n",
    "        if epoch >= 5:\n",
    "            avg += toc - tic\n",
    "        if epoch % eval_every == 0 and epoch != 0:\n",
    "            eval_acc = evaluate(model, val_g, val_nfeat, val_labels, val_nid, device)\n",
    "            print('Eval Acc {:.4f}'.format(eval_acc))\n",
    "            test_acc = evaluate(model, test_g, test_nfeat, test_labels, test_nid, device)\n",
    "            print('Test Acc: {:.4f}'.format(test_acc))\n",
    "\n",
    "    print('Avg epoch time: {}'.format(avg / (epoch - 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(device, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
